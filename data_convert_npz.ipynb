{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a318d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a871db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 사용자 설정 (하이퍼파라미터) ---\n",
    "DATA_PATH = '/dataset/usr012/jmpark/Moudlar_data/data'\n",
    "TRAIN_OUTPUT_PATH = '/dataset/usr012/jmpark/Modular_train'\n",
    "EXPERT_OUTPUT_PATH = '/dataset/usr012/jmpark/Modular_train/expert'\n",
    "\n",
    "# 500만 스텝 중 저장할 스텝 수\n",
    "MAX_STEPS_TO_SAVE = 3_000_000\n",
    "MAX_WORKERS = max(1, os.cpu_count() - 10)\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf917f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_to_npz(file_path, output_dir, max_steps, file_type):\n",
    "    \"\"\"\n",
    "    단일 .csv.gz 파일을 읽어 지정된 포맷의 .npz 파일로 변환합니다.\n",
    "    file_type에 따라 다른 이름 정규화 규칙을 적용합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        # --- [수정 2] : file_type에 따라 다른 패턴 적용 ---\n",
    "        if file_type == 'train':\n",
    "            # Train 파일: _날짜_시간_train 을 통째로 제거\n",
    "            pattern = r'_\\d{8}_\\d{6}_train'\n",
    "            cleaned_filename = re.sub(pattern, '', filename)\n",
    "        elif file_type == 'expert':\n",
    "            # Expert 파일: _날짜_시간 만 제거 (expert는 남김)\n",
    "            pattern = r'_\\d{8}_\\d{6}'\n",
    "            cleaned_filename = re.sub(pattern, '', filename)\n",
    "        else:\n",
    "            # 예외 처리\n",
    "            cleaned_filename = filename\n",
    "            \n",
    "        npz_filename = cleaned_filename.replace('.csv.gz', '.npz')\n",
    "        # --- [여기까지 수정] ---\n",
    "        \n",
    "        output_filepath = os.path.join(output_dir, npz_filename)\n",
    "        \n",
    "        print(f\"[{filename}] 처리 시작...\")\n",
    "        \n",
    "        # 1. CSV 로드\n",
    "        converters = {\n",
    "            'state': ast.literal_eval,\n",
    "            'action': ast.literal_eval,\n",
    "            'next_state': ast.literal_eval\n",
    "        }\n",
    "        use_cols = ['state', 'action', 'next_state', 'reward', 'done_code']\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file_path, \n",
    "                compression='gzip',\n",
    "                nrows=max_steps,\n",
    "                usecols=use_cols,\n",
    "                converters=converters\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"  [오류] {filename} 파일 읽기 실패: {e}\", file=sys.stderr)\n",
    "            return f\"{filename} (실패)\"\n",
    "\n",
    "        # 2. 차원 정보 추출\n",
    "        try:\n",
    "            state_dim = len(df['state'].iloc[0])\n",
    "            action_dim = len(df['action'].iloc[0])\n",
    "            next_state_dim = len(df['next_state'].iloc[0])\n",
    "            \n",
    "            if state_dim != next_state_dim:\n",
    "                print(f\"  [오류] {filename}의 state({state_dim})와 \"\n",
    "                      f\"next_state({next_state_dim}) 차원이 다릅니다.\", file=sys.stderr)\n",
    "                return f\"{filename} (실패)\"\n",
    "                \n",
    "        except (IndexError, TypeError) as e:\n",
    "            print(f\"  [오류] {filename}의 state/action 차원 파악 실패: {e}\", file=sys.stderr)\n",
    "            return f\"{filename} (실패)\"\n",
    "            \n",
    "        # 3. transitions 배열 생성\n",
    "        s_arr = np.array(df['state'].to_list(), dtype=np.float32)\n",
    "        a_arr = np.array(df['action'].to_list(), dtype=np.float32)\n",
    "        ns_arr = np.array(df['next_state'].to_list(), dtype=np.float32)\n",
    "        r_arr = df['reward'].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "        d_arr = df['done_code'].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "        transitions = np.hstack((s_arr, a_arr, ns_arr, r_arr, d_arr))\n",
    "\n",
    "        # 4. NPZ 파일로 압축 저장\n",
    "        np.savez_compressed(\n",
    "            output_filepath,\n",
    "            transitions=transitions,\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim\n",
    "        )\n",
    "        \n",
    "        print(f\"  -> {output_filepath} 저장 완료. (shape={transitions.shape}, s_dim={state_dim}, a_dim={action_dim})\")\n",
    "        return f\"{filename} (성공)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [알 수 없는 오류] {filename} 처리 중 실패: {e}\", file=sys.stderr)\n",
    "        return f\"{filename} (실패)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. 출력 디렉토리 생성 (존재하면 무시)\n",
    "    os.makedirs(TRAIN_OUTPUT_PATH, exist_ok=True)\n",
    "    os.makedirs(EXPERT_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "    # 2. train 파일 변환\n",
    "    train_files = glob.glob(os.path.join(DATA_PATH, '*train.csv.gz'))\n",
    "    if not train_files:\n",
    "        print(f\"경고: '{DATA_PATH}'에서 *train.csv.gz 파일을 찾을 수 없습니다.\")\n",
    "    else:\n",
    "        print(f\"\\n--- 총 {len(train_files)}개의 Train 파일 변환 시작 ---\")\n",
    "        for file in sorted(train_files):\n",
    "            process_csv_to_npz(file, TRAIN_OUTPUT_PATH, MAX_STEPS_TO_SAVE)\n",
    "\n",
    "    # 3. expert 파일 변환\n",
    "    expert_files = glob.glob(os.path.join(DATA_PATH, '*expert.csv.gz'))\n",
    "    if not expert_files:\n",
    "        print(f\"경고: '{DATA_PATH}'에서 *expert.csv.gz 파일을 찾을 수 없습니다.\")\n",
    "    else:\n",
    "        print(f\"\\n--- 총 {len(expert_files)}개의 Expert 파일 변환 시작 ---\")\n",
    "        for file in sorted(expert_files):\n",
    "            process_csv_to_npz(file, EXPERT_OUTPUT_PATH, MAX_STEPS_TO_SAVE)\n",
    "\n",
    "    print(\"\\n--- 모든 작업 완료 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6141897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    os.makedirs(TRAIN_OUTPUT_PATH, exist_ok=True)\n",
    "    os.makedirs(EXPERT_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "    # 1. 처리할 파일 목록을 미리 준비합니다.\n",
    "    train_files = glob.glob(os.path.join(DATA_PATH, '*train.csv.gz'))\n",
    "    expert_files = glob.glob(os.path.join(DATA_PATH, '*expert.csv.gz'))\n",
    "    \n",
    "    total_files = len(train_files) + len(expert_files)\n",
    "    if total_files == 0:\n",
    "        print(f\"경고: '{DATA_PATH}'에서 변환할 *.csv.gz 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"총 {total_files}개의 파일 변환을 시작합니다. (최대 {MAX_WORKERS}개 코어 사용)\")\n",
    "\n",
    "    # 2. ProcessPoolExecutor를 생성합니다.\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        \n",
    "        futures = [] # 작업 목록을 저장할 리스트\n",
    "        \n",
    "        # 3. train 파일 작업을 큐에 추가 (submit)\n",
    "        print(f\"\\n--- {len(train_files)}개의 Train 파일 작업 제출 ---\")\n",
    "        for file in sorted(train_files):\n",
    "            # ★★★ train 파일은 TRAIN_OUTPUT_PATH 로 보냅니다.\n",
    "            futures.append(\n",
    "                executor.submit(process_csv_to_npz, file, TRAIN_OUTPUT_PATH, MAX_STEPS_TO_SAVE)\n",
    "            )\n",
    "\n",
    "        # 4. expert 파일 작업을 큐에 추가 (submit)\n",
    "        print(f\"\\n--- {len(expert_files)}개의 Expert 파일 작업 제출 ---\")\n",
    "        for file in sorted(expert_files):\n",
    "            # ★★★ expert 파일은 EXPERT_OUTPUT_PATH 로 보냅니다.\n",
    "            futures.append(\n",
    "                executor.submit(process_csv_to_npz, file, EXPERT_OUTPUT_PATH, MAX_STEPS_TO_SAVE)\n",
    "            )\n",
    "\n",
    "        # 5. 작업이 완료될 때마다 결과 확인\n",
    "        print(\"\\n--- 작업 처리 시작 (로그가 섞여서 나올 수 있습니다) ---\")\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result() \n",
    "            if \"(성공)\" in result:\n",
    "                success_count += 1\n",
    "            else:\n",
    "                fail_count += 1\n",
    "        \n",
    "        print(f\"\\n--- 모든 작업 완료 (총 {total_files}개 중 {success_count}개 성공, {fail_count}개 실패) ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
